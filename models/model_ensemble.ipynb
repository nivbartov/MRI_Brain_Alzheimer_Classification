{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 35px\">\n",
    "    <img src=\"../assets/icons/icons8-clickteam-fusion-48.png\" style=\"width: 50px; height: 50px; margin-right: 10px;\">\n",
    "    <strong>MRI Brain Alzheimer Classification - Model Ensemble</strong>\n",
    "</h1>\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-git-48.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Clone Repository from GitHub\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Dor\\TECHNION\\deep_learning\\Project\\MRI_Brain_Alzheimer_Classification\n"
     ]
    }
   ],
   "source": [
    "# Clone repository from GitHub\n",
    "# !git clone https://github.com/nivbartov/MRI_Brain_Alzheimer_Classification\n",
    "\n",
    "# %cd MRI_Brain_Alzheimer_Classification\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-import-64.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Import Packages\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, ConcatDataset\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from kornia import augmentation as K\n",
    "from kornia.augmentation import AugmentationSequential\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-function-80.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Import Internal Functions\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import def_models\n",
    "from utils import utils_funcs\n",
    "from utils import optuna_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f'torch.cuda.is_available(): {torch.cuda.is_available()}')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "utils_funcs.open_nvitop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-load-50.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Load Dataset\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torch.load('dataset/dataset_variables/test_set.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-data-recovery-40.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Models Definition and Initialization\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DINOv2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\97252/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize input and output sizes\n",
    "input_width = 224\n",
    "input_height = 224\n",
    "input_channels = 3\n",
    "output_channels = 4\n",
    "\n",
    "dino_v2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "\n",
    "# Freeze DINOv2 layers\n",
    "for param in dino_v2_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create DINOv2 model with FC layers before the output\n",
    "dino_v2_model = def_models.DINOv2(DINOv2_backbone=dino_v2_model, output_channels=output_channels).to(device)\n",
    "\n",
    "pretrained_model_path = f\"./checkpoints/DINOv2_074712_09102024/DINOv2_140224_09102024_train_0.0121_val_0.0937.pth\"\n",
    "checkpoint = torch.load(pretrained_model_path,weights_only=True)\n",
    "dino_v2_model.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EfficientnetB0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize input and output sizes\n",
    "input_width = 224\n",
    "input_height = 224\n",
    "input_channels = 3\n",
    "output_channels = 4\n",
    "\n",
    "# Load a pre-trained EfficientNet-B0 model\n",
    "EfficientNet_backbone_model = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Freeze EfficientNet-B0 layers\n",
    "for param in EfficientNet_backbone_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create EfficientNet-B4 model with FC layers before the output\n",
    "efficientnet_model = def_models.EfficientNet(EfficientNet_backbone=EfficientNet_backbone_model, output_channels=output_channels).to(device)\n",
    "\n",
    "pretrained_model_path = f\"./checkpoints/EfficientNet_083104_17102024/EfficientNet_092843_17102024_train_0.0044_val_0.0401.pth\"\n",
    "checkpoint = torch.load(pretrained_model_path,weights_only=True)\n",
    "efficientnet_model.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-loader-80.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Create Data Loaders\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"display: flex; align-items: center; font-size: 25px; margin: 0;\">\n",
    "    <img src=\"../assets/icons/icons8-unite-64.png\" style=\"width: 40px; height: 40px; margin-right: 5px;\">\n",
    "    Model Ensemble\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "models_group = [dino_v2_model, efficientnet_model]\n",
    "weights = [0.2,0.8]\n",
    "\n",
    "# FGSM parameters\n",
    "epsilon = 0.005\n",
    "\n",
    "# PGD parameters\n",
    "alpha = 0.01\n",
    "num_iter = 5\n",
    "pgd_epsilon = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make the Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy on Natural Images: 99.140%\n"
     ]
    }
   ],
   "source": [
    "ensemble_accuracy = utils_funcs.calculate_ensemble_accuracy(models_group, weights, testloader, device)\n",
    "print(\"Ensemble Model Accuracy on Natural Images: {:.3f}%\".format(ensemble_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_deep_learn_cuda124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
